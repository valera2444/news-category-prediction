{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_params</th>\n",
       "      <th>opt_params</th>\n",
       "      <th>run_number</th>\n",
       "      <th>epoch</th>\n",
       "      <th>validation_or_train</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>additional_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, model_params, opt_params, run_number, epoch, validation_or_train, accuracy, loss, additional_info]\n",
       "Index: []"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "stat_file = pd.DataFrame({'model':[],\n",
    "                  'model_params':[],\n",
    "                  'opt_params':[],\n",
    "                  'run_number':[],\n",
    "                  'epoch':[],\n",
    "                  'validation_or_train':[],\n",
    "                  'accuracy':[],\n",
    "                  'loss':[],\n",
    "                'additional_info':[]})\n",
    "stat_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "j=pd.read_json('./data.json', lines=True)\n",
    "j.to_csv('./data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/valeriy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/valeriy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/valeriy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/valeriy/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/valeriy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_sentence(sent):\n",
    "    lemmatized = []\n",
    "    for word, tag in pos_tag(word_tokenize(sent)):\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "        if not wntag:\n",
    "            lemma = word\n",
    "        else:\n",
    "            lemma = wnl.lemmatize(word, wntag)\n",
    "\n",
    "        lemmatized.append(lemma)\n",
    "\n",
    "    #print(' '.join(lemmatized))\n",
    "    return ' '.join(lemmatized)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "def clean(row, is_lemmatize):\n",
    "    \n",
    "    row = row.lower()\n",
    "    row = re.sub('[^a-z A-Z 0-9]+', ' ', row)\n",
    "    row = \" \".join([word for word in row.split() if word not in stopwords.words('english')])\n",
    "    if is_lemmatize:\n",
    "        row = lemmatize_sentence(row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(j, is_lemmatize=False):\n",
    "    j['headline'] = j['headline'].apply(clean,is_lemmatize=is_lemmatize)\n",
    "    j['short_description'] = j['short_description'].apply(clean,is_lemmatize=is_lemmatize)\n",
    "    j['link'] = j['link'].apply(clean,is_lemmatize=is_lemmatize)\n",
    "    j ['authors'] = j['authors'].apply(clean,is_lemmatize=is_lemmatize)\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = clean_dataframe(j,is_lemmatize=True)\n",
    "data_cleaned.to_csv('data_cleaned_full_lemmatized.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "link                     0\n",
       "headline                67\n",
       "category                 0\n",
       "short_description    20179\n",
       "authors              37419\n",
       "date                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_cleaned = pd.read_csv('data_cleaned_not_lemmatized.csv')\n",
    "data_cleaned.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nans are here!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = ['headline', 'short_description', 'authors', 'link']\n",
    "for col in columns_to_convert:\n",
    "    data_cleaned[col] = data_cleaned[col].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "link                 https www huffpost com entry american airlines...\n",
       "headline             american airlines flyer charged banned life pu...\n",
       "category                                                     U.S. NEWS\n",
       "short_description    subdued passengers crew fled back aircraft con...\n",
       "authors                                                 mary papenfuss\n",
       "date                                                        2022-09-23\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concat_f = lambda a : ' '.join(a)\n",
    "grouped = data_cleaned.groupby('category').agg({'headline':concat_f, 'short_description':concat_f, 'authors':concat_f, 'link':concat_f}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35, 35,  5, ..., 28, 28, 28])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "data_labels = le.fit_transform(data_cleaned.category)\n",
    "data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(model, data, features=['headline','short_description' ]):\n",
    "    feature=''\n",
    "    for el in features:\n",
    "        feature+=data[el]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(feature,\n",
    "                                                        data_labels, \n",
    "                                                        train_size=0.8, test_size=0.2,\n",
    "                                                        random_state=42)\n",
    "    \n",
    "    \n",
    "    cv = TfidfVectorizer ()\n",
    "    X_train = cv.fit_transform(x_train)\n",
    "    X_test = cv.transform(x_test)\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    f1, accuracy = f1_score(preds, y_test, average='macro'), accuracy_score(preds, y_test)\n",
    "    print(f1, accuracy)\n",
    "\n",
    "    return f1, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_row(df, row):\n",
    "    return pd.concat([df, row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37950420278653324 0.549634897150766\n"
     ]
    }
   ],
   "source": [
    "nb = ComplementNB()\n",
    "feature_list=['headline','short_description' ]\n",
    "f1, accuracy = run_pipeline(nb, data_cleaned,feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = {'model': nb.__class__,\n",
    "        'model_params':None,\n",
    "        'opt_params':None,\n",
    "        'run_number':None,\n",
    "        'epoch':None,\n",
    "        'validation_or_train':'validation',\n",
    "        'accuracy':accuracy,\n",
    "        'loss':None,\n",
    "          'additional_info':'no lemmatization '+'features: ' + ' '.join(feature_list)}\n",
    "\n",
    "stat_file = insert_row(stat_file, pd.DataFrame(row, index=[len(stat_file)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_file.to_csv('MNB-tfidf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_category_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
